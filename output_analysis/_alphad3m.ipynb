{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:05.345007Z",
     "start_time": "2023-05-13T16:11:05.302970Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import graphviz\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:05.641178Z",
     "start_time": "2023-05-13T16:11:05.597067Z"
    }
   },
   "outputs": [],
   "source": [
    "kc1 = \"alphad3m-ds_splice-seed_662873\"\n",
    "temp =  \"temp\"\n",
    "sqlite_db =  \"db.sqlite3\"\n",
    "task_grammar = \"task_grammar.bnf\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:05.867156Z",
     "start_time": "2023-05-13T16:11:05.823662Z"
    }
   },
   "outputs": [],
   "source": [
    "def connect_to_sqlite(path: str) -> (sqlite3.Connection, sqlite3.Cursor):\n",
    "    \"\"\"\n",
    "    connects and returns connections and cursor for the sqlight3 database from the path\n",
    "    :param path: str full path of the sql db\n",
    "    :return: connection, cursor\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(path):\n",
    "\n",
    "        conn = sqlite3.connect(path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        return conn, cursor\n",
    "\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# conn, cursor = connect_to_sqlite(sqlite_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:05.925065Z",
     "start_time": "2023-05-13T16:11:05.884633Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_rows(cursor:sqlite3.Cursor,\n",
    "                sql_query: str = \"\"\"SELECT name FROM sqlite_master WHERE type='table';\"\"\"\n",
    "                ) -> list:\n",
    "    \n",
    "    cursor = cursor.execute(sql_query)\n",
    "    record = cursor.fetchall()\n",
    "    return cursor, record\n",
    "\n",
    "\n",
    "# _, table_names = fetch_rows(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:06.409341Z",
     "start_time": "2023-05-13T16:11:06.364350Z"
    }
   },
   "outputs": [],
   "source": [
    "pipelines_q = \"\"\"SELECT\n",
    "                    pipelines.id AS id,\n",
    "                    pipelines.origin AS pipeline\n",
    "                FROM\n",
    "                    pipelines\n",
    "                \"\"\"\n",
    "# pipelines = pd.read_sql_query(pipelines_q, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:06.549129Z",
     "start_time": "2023-05-13T16:11:06.506984Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_scores_q = \"\"\"\n",
    "                SELECT evaluation_id, AVG(value) AS avg_score\n",
    "                FROM  evaluation_scores\n",
    "                GROUP BY evaluation_id\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "# scores = pd.read_sql_query(avg_scores_q, conn)\n",
    "# avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:06.758524Z",
     "start_time": "2023-05-13T16:11:06.720939Z"
    }
   },
   "outputs": [],
   "source": [
    "# pipelines = pipelines.join(avg_scores.set_index(\"evaluation_id\"), on=\"evaluation_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:06.812168Z",
     "start_time": "2023-05-13T16:11:06.759372Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_pipeline_as_list(df:pd.DataFrame, col:str=\"pipeline\") -> pd.DataFrame:\n",
    "    def from_str_pipeline_to_list(p):\n",
    "        # if \"Template\" in p:\n",
    "        #     slice_start = 10\n",
    "        # elif \"AlphaAutoML\" in p:\n",
    "        #     slice_start = 13\n",
    "        # p = p[slice_start:-1]\n",
    "\n",
    "\n",
    "\n",
    "        p = p.strip(\"Template\")\n",
    "        p = p.strip(\"AlphaAutoML\")\n",
    "        p = p.strip()\n",
    "        p = p.strip(\"()\")\n",
    "        p = p.split(\",\")\n",
    "        return list(map(lambda x: \"d3m.primitives.\" + x.strip(), p))\n",
    "    df[col] = df[col].map(from_str_pipeline_to_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:07.049054Z",
     "start_time": "2023-05-13T16:11:07.005967Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# pipelines= make_pipeline_as_list(pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:07.241000Z",
     "start_time": "2023-05-13T16:11:07.195947Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_grammar_as_list(path: str) -> list:\n",
    "\n",
    "    with open(path, \"r\") as infile:\n",
    "        return infile.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:07.441175Z",
     "start_time": "2023-05-13T16:11:07.440943Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# grammar_list = read_grammar_as_list(task_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:07.609097Z",
     "start_time": "2023-05-13T16:11:07.608925Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_component_from_p_rule(line:str) -> str:\n",
    "    split_line = line.split(\" \")\n",
    "    component = split_line[-1]\n",
    "    component = component.strip(\"'\")\n",
    "    if component == \"E\":\n",
    "        return None\n",
    "    return component\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_component_type_lists_from_grammar(grammar:list) -> (list, list, list):\n",
    "\n",
    "\n",
    "    # alphad3m/resource/primitives_hierarchy.json\n",
    "\n",
    "    encoders_list_from_source_code = [\n",
    "        \"d3m.primitives.feature_extraction.count_vectorizer.SKlearn\",\n",
    "        \"d3m.primitives.feature_extraction.tfidf_vectorizer.SKlearn\",\n",
    "        \"d3m.primitives.data_transformation.encoder.DistilTextEncoder\",\n",
    "        \"d3m.primitives.feature_construction.corex_text.DSBOX\",\n",
    "        \"d3m.primitives.data_transformation.one_hot_encoder.SKlearn\",\n",
    "        \"d3m.primitives.data_transformation.encoder.DSBOX\",\n",
    "        \"d3m.primitives.data_transformation.ordinal_encoder.SKlearn\",\n",
    "        \"d3m.primitives.data_transformation.label_decoder.Common\",\n",
    "        \"d3m.primitives.data_transformation.label_encoder.Common\",\n",
    "        \"d3m.primitives.data_cleaning.label_encoder.DSBOX\",\n",
    "        \"d3m.primitives.data_transformation.enrich_dates.DistilEnrichDates\",\n",
    "        \"d3m.primitives.data_transformation.encoder.DistilBinaryEncoder\",\n",
    "        \"d3m.primitives.data_transformation.replace_singletons.DistilReplaceSingletons\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pipeline_template = None\n",
    "    encoders=  encoders_list_from_source_code\n",
    "    imputation = []\n",
    "    feat_select = []\n",
    "    feat_scale  =[]\n",
    "    clfs = []\n",
    "\n",
    "    imputation.append(\"d3m.primitives.data_cleaning.imputer.SKlearn\")\n",
    "\n",
    "    for line in grammar:\n",
    "\n",
    "        if (\"S ->\" in line) and (not \"ENCODERS ->\" in line):\n",
    "            template = line.split(\" \")\n",
    "            template = template[2:]# remove \"S ->\"\n",
    "            template = list(map(lambda x: x.strip(\"'\") , template))\n",
    "            pipeline_template = template\n",
    "\n",
    "        else:\n",
    "            if (\"ENCODERS\" in line) or (\"CATEGORICAL_ENCODER\" in line):\n",
    "                c = get_component_from_p_rule(line)\n",
    "                if c != \"CATEGORICAL_ENCODER\":\n",
    "                    encoders.append(c)\n",
    "            elif \"IMPUTATION\" in line:\n",
    "                c = get_component_from_p_rule(line)\n",
    "                imputation.append(c)\n",
    "            elif \"FEATURE_SELECTION\" in line:\n",
    "                c = get_component_from_p_rule(line)\n",
    "                feat_select.append(c)\n",
    "            elif \"FEATURE_SCALING\" in line:\n",
    "                c = get_component_from_p_rule(line)\n",
    "                feat_scale.append(c)\n",
    "            elif \"CLASSIFICATION\" in line:\n",
    "                c = get_component_from_p_rule(line)\n",
    "                clfs.append(c)\n",
    "            else:\n",
    "                print(f\"Something is wrong? {line}\")\n",
    "\n",
    "\n",
    "    lambda_is_none = lambda x: x is not None\n",
    "\n",
    "    encoders = list(filter(lambda_is_none, encoders))\n",
    "    imputation = list(filter(lambda_is_none, imputation))\n",
    "    feat_select = list(filter(lambda_is_none, feat_select))\n",
    "    feat_scale = list(filter(lambda_is_none, feat_scale))\n",
    "\n",
    "\n",
    "    return pipeline_template, encoders, imputation, feat_select, feat_scale, clfs\n",
    "\n",
    "\n",
    "# pipeline_template, encoders, imputation, feat_select, feat_scale, clfs = get_component_type_lists_from_grammar(grammar_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:08.093129Z",
     "start_time": "2023-05-13T16:11:08.092898Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# pipelines[\"n_components\"] = pipelines[\"pipeline\"].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:08.273210Z",
     "start_time": "2023-05-13T16:11:08.226962Z"
    }
   },
   "outputs": [],
   "source": [
    "f_select_names = {\n",
    "    \"d3m.primitives.data_transformation.to_numeric.DSBOX\":\"ToNumeric (DSBOX)\",\n",
    "    \"d3m.primitives.feature_selection.generic_univariate_select.SKlearn\": \"GenericUnivariateSelect\",\n",
    "    \"d3m.primitives.feature_selection.select_percentile.SKlearn\": \"SelectPercentile\",\n",
    "    \"d3m.primitives.feature_selection.variance_threshold.SKlearn\" : \"VarianceThreshold\",\n",
    "    \"d3m.primitives.feature_selection.select_fwe.SKlearn\" : \"SelectFwe\"}\n",
    "\n",
    "f_scale_names = {\n",
    "    \"d3m.primitives.data_cleaning.max_abs_scaler.SKlearn\": \"MaxAbsScaler\",\n",
    "    \"d3m.primitives.data_cleaning.min_max_scaler.SKlearn\": \"MinMaxScaler\",\n",
    "    \"d3m.primitives.data_cleaning.quantile_transformer.SKlearn\": \"QuantileTransformer\",\n",
    "    \"d3m.primitives.data_cleaning.robust_scaler.SKlearn\": \"RobustScaler\",\n",
    "    \"d3m.primitives.data_cleaning.standard_scaler.SKlearn\": \"StandardScaler\",\n",
    "    \"d3m.primitives.normalization.iqr_scaler.DSBOX\": \"IQRScaler (DSBOX)\"\n",
    "}\n",
    "\n",
    "clfs_names ={\n",
    "    \"d3m.primitives.classification.ada_boost.SKlearn\": \"AdaBoostingClassifier\",\n",
    "    \"d3m.primitives.classification.bagging.SKlearn\": \"BaggingClassifier\",\n",
    "    \"d3m.primitives.classification.bernoulli_naive_bayes.SKlearn\": \"BernoulliNB\",\n",
    "    \"d3m.primitives.classification.decision_tree.SKlearn\" : \"DecisionTreeClassifier\",\n",
    "    \"d3m.primitives.classification.extra_trees.SKlearn\": \"ExtraTreesClassifier\",\n",
    "    \"d3m.primitives.classification.gaussian_naive_bayes.SKlearn\": \"GaussianNB\",\n",
    "    \"d3m.primitives.classification.gradient_boosting.SKlearn\" : \"GradientBoostingClassifier\",\n",
    "    \"d3m.primitives.classification.k_neighbors.SKlearn\": \"KNeighborsClassifier\",\n",
    "    \"d3m.primitives.classification.linear_discriminant_analysis.SKlearn\": \"LinearDiscriminantAnalysis\",\n",
    "    \"d3m.primitives.classification.linear_svc.SKlearn\": \"LinearSVC\",\n",
    "    \"d3m.primitives.classification.logistic_regression.SKlearn\": \"LogisticRegression\",\n",
    "    \"d3m.primitives.classification.mlp.SKlearn\": \"MLPClassifier\",\n",
    "    \"d3m.primitives.classification.multinomial_naive_bayes.SKlearn\": \"MultinomialNB\",\n",
    "    \"d3m.primitives.classification.nearest_centroid.SKlearn\" : \"NearestCentroid\",\n",
    "    \"d3m.primitives.classification.passive_aggressive.SKlearn\": \"PassiveAggressiveClassifier\",\n",
    "    \"d3m.primitives.classification.quadratic_discriminant_analysis.SKlearn\": \"QuadraticDiscriminantAnalysis\",\n",
    "    \"d3m.primitives.classification.random_forest.SKlearn\": \"RandomForestClassifier\",\n",
    "    \"d3m.primitives.classification.sgd.SKlearn\": \"SGDClassifier\",\n",
    "    \"d3m.primitives.classification.svc.SKlearn\" : \"SVC\",\n",
    "    \"d3m.primitives.classification.Convolutional_neural_network.Fastai\" : \"ConvolutionalNN (FasAI)\",\n",
    "    \"d3m.primitives.classification.light_gbm.Common\": \"LGBMClassifier (LightGBM)\",\n",
    "    \"d3m.primitives.classification.xgboost_dart.Common\" : \"XGBoostClassifierDart (XGBoost)\",\n",
    "    \"d3m.primitives.classification.xgboost_gbtree.Common\" : \"XGBoostClassifierGBTree (XGBoost)\",\n",
    "}\n",
    "\n",
    "imputer_names = {\n",
    "    \"d3m.primitives.data_cleaning.imputer.SKlearn\": \"Imputer\"\n",
    "}\n",
    "\n",
    "encoder_names = {\n",
    "    \"d3m.primitives.data_transformation.one_hot_encoder.SKlearn\" : \"OneHotEncoder\",\n",
    "    \"d3m.primitives.data_transformation.encoder.DSBOX\": \"Encoder (DSBOX)\",\n",
    "    \"d3m.primitives.feature_extraction.count_vectorizer.SKlearn\": \"CountVectorizer\",\n",
    "    \"d3m.primitives.feature_extraction.tfidf_vectorizer.SKlearn\": \"TfidfVectorizer\",\n",
    "    \"d3m.primitives.data_transformation.encoder.DistilTextEncoder\": \"TextEncoder (Distil)\", # !!\n",
    "    \"d3m.primitives.feature_construction.corex_text.DSBOX\" : \"CorexText (DSBOX)\",\n",
    "    \"d3m.primitives.data_transformation.ordinal_encoder.SKlearn\": \"OrdinalEncoder\",\n",
    "    \"d3m.primitives.data_cleaning.label_encoder.DSBOX\": \"LabelEncoder (DSBOX)\",\n",
    "    \"d3m.primitives.data_transformation.enrich_dates.DistilEnrichDates\": \"EnrichDates (Distil)\",\n",
    "    \"d3m.primitives.data_transformation.replace_singletons.DistilReplaceSingletons\": \"ReplaceSingletons (Distil)\",\n",
    "    \"d3m.primitives.data_transformation.label_encoder.Common\": \"LabelEncoder\",\n",
    "    \"d3m.primitives.data_transformation.label_decoder.Common\": \"LabelDecoder\",\n",
    "    \"d3m.primitives.data_transformation.encoder.DistilBinaryEncoder\": \"BinaryEncoder (Distil)\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "names = [encoder_names, clfs_names, imputer_names, f_select_names, f_scale_names]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:08.697165Z",
     "start_time": "2023-05-13T16:11:08.696976Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_pipeline_c_names(pipeline):\n",
    "    def mapper(component):\n",
    "        for n in names:\n",
    "            if n.get(component):\n",
    "                component = n[component]\n",
    "                break\n",
    "        return component\n",
    "\n",
    "    return list(map(mapper, pipeline))\n",
    "\n",
    "\n",
    "# pipelines[\"pipeline\"] = pipelines[\"pipeline\"].map(map_pipeline_c_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:09.061160Z",
     "start_time": "2023-05-13T16:11:09.012500Z"
    }
   },
   "outputs": [],
   "source": [
    "def mapper(component):\n",
    "    for n in names:\n",
    "        if n.get(component):\n",
    "            component = n[component]\n",
    "            break\n",
    "    return component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:09.345017Z",
     "start_time": "2023-05-13T16:11:09.301581Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_component_types_and_counts_cols(df : pandas.DataFrame, types : dict) -> pandas.DataFrame:\n",
    "\n",
    "    def count_components_of_type(row: pandas.Series, check_list:list) -> pd.Series:\n",
    "        components_of_type = list(filter(lambda x: x in check_list, row[\"pipeline\"]))\n",
    "        return pd.Series([components_of_type])\n",
    "\n",
    "    for k, v in types.items():\n",
    "        df[k] = df.apply(count_components_of_type, args=(v,), axis = 1)\n",
    "        df[f\"n_{k}\"] = df[k].map(len)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:10.049065Z",
     "start_time": "2023-05-13T16:11:10.004937Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# pipelines = get_component_types_and_counts_cols(pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:10.173114Z",
     "start_time": "2023-05-13T16:11:10.130688Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_edges(pipeline):\n",
    "    edges = []\n",
    "    edges.append([\"Input Data\", pipeline[0]])\n",
    "    for ix in range(0, len(pipeline)-1):\n",
    "        edges.append([pipeline[ix], pipeline[ix+1]])\n",
    "    return edges\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pipelines[\"edges\"] = pipelines[\"pipeline\"].map(make_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:10.445024Z",
     "start_time": "2023-05-13T16:11:10.404133Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize(edges: list, out_name: str ,\n",
    "              save_dir: str = None,\n",
    "              graph_attrs: dict=None,\n",
    "              view: bool = False,\n",
    "              format=\"pdf\") -> graphviz.Digraph:\n",
    "\n",
    "    if graph_attrs is None:\n",
    "        graph_attr= {'rankdir':'LR'}\n",
    "\n",
    "    dot = graphviz.Digraph(out_name,\n",
    "                      graph_attr=graph_attr)\n",
    "    dot.format = format\n",
    "    dot.edge_attr.update(arrowhead='vee', arrowsize='1.4')\n",
    "\n",
    "    nodes = itertools.chain(*edges)\n",
    "    for n in nodes:\n",
    "        if \"Input Data\" in n:\n",
    "            dot.node(n,  shape=\"cylinder\", height=\"1.1\")\n",
    "        else:\n",
    "            dot.node(n, height = \"1.1\")\n",
    "\n",
    "    dot.edges(edges)\n",
    "\n",
    "\n",
    "    dot.render(directory=save_dir, view=view)\n",
    "\n",
    "\n",
    "\n",
    "# visualize(pipelines[\"edges\"].iloc[0],\"test\",  os.path.join(kc1, \"pipeline_vis\"), format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"/home/hadi/PycharmProjects/Master-Thesis/zzz_automl_outputs/alphad3m-ds_splice-seed_662873\"\n",
    "temp = os.path.join(d, \"temp\")\n",
    "sqlite_db = os.path.join(d, \"db.sqlite3\")\n",
    "task_grammar = os.path.join(d, \"task_grammar.bnf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, cursor = connect_to_sqlite(sqlite_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = pd.read_sql_query(pipelines_q, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = pd.read_sql_query(\"SELECT * FROM evaluation_scores\", conn)\n",
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "pipelines= make_pipeline_as_list(pipelines)\n",
    "grammar_list = read_grammar_as_list(task_grammar)\n",
    "pipeline_template, encoders, imputation, feat_select, feat_scale, clfs = get_component_type_lists_from_grammar(grammar_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoders = list(map(mapper, encoders))\n",
    "imputation = list(map(mapper, imputation))\n",
    "\n",
    "feat_select = list(map(mapper, feat_select))\n",
    "\n",
    "feat_scale = list(map(mapper, feat_scale))\n",
    "\n",
    "clfs = list(map(mapper, clfs))\n",
    "\n",
    "\n",
    "types = {\n",
    "    \"encoders\": encoders,\n",
    "    \"imputation\":imputation,\n",
    "    \"feat_select\": feat_select,\n",
    "    \"feat_scale\": feat_scale,\n",
    "    \"clfs\": clfs\n",
    "}\n",
    "\n",
    "\n",
    "pipelines[\"n_components\"] = pipelines[\"pipeline\"].map(lambda x: len(x))\n",
    "pipelines[\"pipeline\"] = pipelines[\"pipeline\"].map(map_pipeline_c_names)\n",
    "\n",
    "\n",
    "pipelines = get_component_types_and_counts_cols(pipelines, types)\n",
    "pipelines[\"edges\"] = pipelines[\"pipeline\"].map(make_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:11.337150Z",
     "start_time": "2023-05-13T16:11:11.295566Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_summary_and_visualize(\n",
    "    d_name:str,\n",
    "    ROOT : str = \"/home/hadi/PycharmProjects/Master-Thesis/automl_outputs/gpuserver/alphad3m\") -> pandas.DataFrame :\n",
    "\n",
    "    d = os.path.join(ROOT, d_name)\n",
    "    temp = os.path.join(d, \"temp\")\n",
    "    sqlite_db = os.path.join(temp, \"db.sqlite3\")\n",
    "    task_grammar = os.path.join(temp, \"task_grammar.bnf\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    conn, cursor = connect_to_sqlite(sqlite_db)\n",
    "    pipelines = pd.read_sql_query(pipelines_q, conn)\n",
    "    conn.close()\n",
    "    pipelines= make_pipeline_as_list(pipelines)\n",
    "    grammar_list = read_grammar_as_list(task_grammar)\n",
    "    pipeline_template, encoders, imputation, feat_select, feat_scale, clfs = get_component_type_lists_from_grammar(grammar_list)\n",
    "\n",
    "    encoders = list(map(mapper, encoders))\n",
    "    imputation = list(map(mapper, imputation))\n",
    "\n",
    "    feat_select = list(map(mapper, feat_select))\n",
    "\n",
    "    feat_scale = list(map(mapper, feat_scale))\n",
    "\n",
    "    clfs = list(map(mapper, clfs))\n",
    "\n",
    "\n",
    "    types = {\n",
    "        \"encoders\": encoders,\n",
    "        \"imputation\":imputation,\n",
    "        \"feat_select\": feat_select,\n",
    "        \"feat_scale\": feat_scale,\n",
    "        \"clfs\": clfs\n",
    "    }\n",
    "\n",
    "\n",
    "    pipelines[\"n_components\"] = pipelines[\"pipeline\"].map(lambda x: len(x))\n",
    "    pipelines[\"pipeline\"] = pipelines[\"pipeline\"].map(map_pipeline_c_names)\n",
    "\n",
    "\n",
    "    pipelines = get_component_types_and_counts_cols(pipelines, types)\n",
    "    pipelines[\"edges\"] = pipelines[\"pipeline\"].map(make_edges)\n",
    "    for ix, r in pipelines.iterrows():\n",
    "\n",
    "        visualize(r[\"edges\"], r[\"id\"],  os.path.join(d, \"pipeline_vis\"), format=\"png\")\n",
    "\n",
    "    return pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:11:27.265399Z",
     "start_time": "2023-05-13T16:11:12.543099Z"
    }
   },
   "outputs": [],
   "source": [
    "kc1_pipelines = get_summary_and_visualize(kc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:12:13.749666Z",
     "start_time": "2023-05-13T16:12:13.737559Z"
    }
   },
   "outputs": [],
   "source": [
    "kc1_pipelines[\"id\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:13:08.136131Z",
     "start_time": "2023-05-13T16:13:08.123269Z"
    }
   },
   "outputs": [],
   "source": [
    "kc1_pipelines.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
