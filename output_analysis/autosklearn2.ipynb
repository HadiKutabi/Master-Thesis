{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Sklearn Output Analysis\n",
    "\n",
    "## Overview<a id=ov>\n",
    "1. [Functions](#funcs)<br>\n",
    "2. [Auto-Sklearn 1.0](#ask1)<br>\n",
    "    2.1 [kc1](#1_kc1)<br>\n",
    "    2.2 [electrictiy](#2_elec)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:37.194784Z",
     "start_time": "2023-05-17T14:16:37.152235Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:37.410336Z",
     "start_time": "2023-05-17T14:16:37.363495Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "from os.path import join as pjoin\n",
    "import graphviz\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import sklearn.metrics\n",
    "\n",
    "from utils.helpers import get_project_root\n",
    "import pickle\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:37.536321Z",
     "start_time": "2023-05-17T14:16:37.482565Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT = get_project_root()\n",
    "OUTPUTS = pjoin(ROOT, \"automl_outputs\")\n",
    "SEED = 662873\n",
    "\n",
    "DIR_TEMPLATE = pjoin(\n",
    "    OUTPUTS,\n",
    "    \"autosklearn2-ds_{d_name}-seed_{seed}\"\n",
    ")\n",
    "\n",
    "RUN_HISTORY = pjoin(DIR_TEMPLATE, \"smac3-output/run_{seed}/runhistory.json\")\n",
    "\n",
    "AUTOML_OBJ = pjoin(DIR_TEMPLATE, \"autosklearn_obj.pkl\")\n",
    "\n",
    "VISUALS = pjoin(DIR_TEMPLATE, \"visuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functions<a id=funcs>\n",
    "\n",
    "[back to overview](#ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:37.802099Z",
     "start_time": "2023-05-17T14:16:37.750619Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_run_history(path):\n",
    "    with open(path, \"r\") as infile:\n",
    "        return json.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:37.928542Z",
     "start_time": "2023-05-17T14:16:37.878431Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_run_df(rh: dict) -> pandas.DataFrame:\n",
    "    id_ = []\n",
    "    task_id = []\n",
    "    status = []\n",
    "    num_run = []\n",
    "    config_origin = []\n",
    "\n",
    "    run_df = pd.DataFrame()\n",
    "\n",
    "    for i in rh[\"data\"]:\n",
    "        id_.append(i[0][0])\n",
    "        task_id.append(json.loads(i[0][1])[\"task_id\"]) # dict is saved as a string\n",
    "        status.append(i[1][2][\"__enum__\"])\n",
    "\n",
    "        if (status[-1] == \"StatusType.TIMEOUT\" ) or (status[-1] == \"StatusType.CRASHED\") or (status[-1] == \"StatusType.STOP\") or (status[-1] == \"StatusType.MEMOUT\"):\n",
    "            num_run.append(None)\n",
    "        else:\n",
    "            num_run.append(int(i[1][5][\"num_run\"]))\n",
    "\n",
    "        if (status[-1] == \"StatusType.STOP\"):\n",
    "\n",
    "            config_origin.append(None)\n",
    "        else:\n",
    "            config_origin.append(i[1][5][\"configuration_origin\"])\n",
    "\n",
    "\n",
    "    run_df[\"id\"] = id_\n",
    "    run_df[\"task_id\"] = task_id\n",
    "    run_df[\"status\"] = status\n",
    "    run_df[\"num_run\"] = num_run\n",
    "    run_df[\"config_origin\"] = config_origin\n",
    "\n",
    "    return run_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:38.185801Z",
     "start_time": "2023-05-17T14:16:38.171282Z"
    }
   },
   "outputs": [],
   "source": [
    "clfs_names_map = {\n",
    "    \"adaboost\": \"AdaBoostingClassifier\",\n",
    "    \"bernoulli_nb\": \"BernoulliNB\",\n",
    "    \"decision_tree\": \"DecisionTreeClassifier\",\n",
    "    \"extra_trees\": \"ExtraTreesClassifier\",\n",
    "    \"gaussian_nb\" : \"GaussianNB\",\n",
    "    \"gradient_boosting\" : \"GradientBoostingClassifier\",\n",
    "    \"k_nearest_neighbors\" : \"KNeighborsClassifier\",\n",
    "    \"lda\" : \"LinearDiscriminantAnalysis\",\n",
    "    \"liblinear_svc\" : \"LinearSVC\",\n",
    "    \"libsvm_svc\" : \"SVC\",\n",
    "    \"mlp\" : \"MLPClassifier\",\n",
    "    \"multinomial_nb\" : \"MultinomialNB\",\n",
    "    \"passive_aggressive\": \"PassiveAggressiveClassifier\",\n",
    "    \"qda\" : \"QuadraticDiscriminantAnalysis\",\n",
    "    \"random_forest\" : \"RandomForestClassifier\",\n",
    "    \"sgd\" : \"SGDClassifier\"\n",
    "\n",
    "}\n",
    "\n",
    "feat_preproc_name_map = {\n",
    "    \"extra_trees_preproc_for_classification\": \"SelectFeatsFromExtraTrees\",\n",
    "    \"fast_ica\": \"FastICA\",\n",
    "    \"feature_agglomeration\": \"FeatureAgglomeration\",\n",
    "    \"kernel_pca\":\"KernelPCA\",\n",
    "    \"kitchen_sinks\": \"RBFSampler\",\n",
    "    \"liblinear_svc_preprocessor\": \"SelectFeatsFromLinearSVC\",\n",
    "    \"no_preprocessing\": \"no_preprocessing\",\n",
    "    \"nystroem_sampler\": \"Nystroem\",\n",
    "    \"pca\": \"PCA\",\n",
    "    \"polynomial\": \"PolynomialFeatures\",\n",
    "    \"random_trees_embedding\": \"RandomTreesEmbedding\",\n",
    "    \"select_percentile_classification\": \"SelectPercentile\",\n",
    "    \"select_rates_classification\" : \"SelectRate\"\n",
    "}\n",
    "\n",
    "cat_data_preproc_name_map = {\n",
    "    \"category_coalescence\": \"CategoryCoalescence\",\n",
    "    \"one_hot_encoding\" : \"OneHotEncoder\",\n",
    "    \"encoding\" : \"OrdinalEncoder\"\n",
    "}\n",
    "\n",
    "text_data_preproc_name_map = {\n",
    "    \"text_feature_reduction\": \"TruncatedSVD\",\n",
    "    \"tfidf_encoding\" : \"TfidfVectorizer\"\n",
    "}\n",
    "\n",
    "\n",
    "mum_data_preproc_name_map = {\n",
    "    \"impute\": \"Imputation\",\n",
    "    \"minmax\" : \"MinMaxScaler\",\n",
    "    \"normalize\" : \"Normalizer\",\n",
    "    \"power_transformer\" : \"PowerTransformer\",\n",
    "    \"quantile_transformer\": \"QuantileTransformer\",\n",
    "    \"robust_scaler\" : \"RobustScaler\",\n",
    "    \"standardize\" : \"StandardScaler\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:38.356803Z",
     "start_time": "2023-05-17T14:16:38.310931Z"
    }
   },
   "outputs": [],
   "source": [
    "def substitute_component_name(name : str, mapping_dict: dict )-> str:\n",
    "    return mapping_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:38.546744Z",
     "start_time": "2023-05-17T14:16:38.504636Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_classifier(conf:dict, names_map: dict = clfs_names_map) -> str:\n",
    "    return substitute_component_name(conf[\"classifier:__choice__\"], names_map)\n",
    "\n",
    "def get_feat_preproc(conf:dict, f_preproc_names_map: dict = feat_preproc_name_map)-> str:\n",
    "    feat_preproc = conf[ \"feature_preprocessor:__choice__\"]\n",
    "    if feat_preproc == \"no_preprocessing\":\n",
    "        return None\n",
    "    else:\n",
    "        return substitute_component_name(feat_preproc, f_preproc_names_map)\n",
    "\n",
    "\n",
    "def get_cat_data_preprocs(conf:dict, cat_preproc_name_map : dict = cat_data_preproc_name_map)-> list:\n",
    "    steps = []\n",
    "    category_coalescence = conf.get(\"data_preprocessor:feature_type:categorical_transformer:category_coalescence:__choice__\")\n",
    "    if (category_coalescence is not None) and (category_coalescence != \"no_coalescence\"):\n",
    "        steps.append(substitute_component_name(\"category_coalescence\", cat_preproc_name_map))\n",
    "\n",
    "    encoding = conf.get(\"data_preprocessor:feature_type:categorical_transformer:categorical_encoding:__choice__\")\n",
    "    if (encoding != \"no_encoding\") and (encoding is not None):\n",
    "        \n",
    "        steps.append(substitute_component_name(encoding, cat_preproc_name_map))\n",
    "\n",
    "    return steps\n",
    "\n",
    "def get_text_data_preprocs(conf:dict, text_preproc_name_map: dict = text_data_preproc_name_map)-> list:\n",
    "    steps = []\n",
    "\n",
    "    text_encoding = conf.get(\"data_preprocessor:feature_type:text_transformer:text_encoding:__choice__\")\n",
    "    if text_encoding:\n",
    "        steps.append(substitute_component_name(text_encoding, text_preproc_name_map))\n",
    "\n",
    "    text_feature_reduction = conf.get(\"data_preprocessor:feature_type:text_transformer:text_feature_reduction:n_components\")\n",
    "    if text_feature_reduction:\n",
    "        steps.append(substitute_component_name(\"text_feature_reduction\", text_preproc_name_map))\n",
    "\n",
    "    return steps\n",
    "\n",
    "\n",
    "def get_num_data_preprocs(conf:dict, num_preproc_name_map : dict = mum_data_preproc_name_map)-> list:\n",
    "    steps = []\n",
    "\n",
    "    imputation = conf.get(\"data_preprocessor:feature_type:numerical_transformer:imputation:strategy\")\n",
    "    if imputation:\n",
    "        imputation = substitute_component_name(\"impute\", num_preproc_name_map)\n",
    "        steps.append(imputation)\n",
    "\n",
    "    scaling = conf.get(\"data_preprocessor:feature_type:numerical_transformer:rescaling:__choice__\")\n",
    "    if (scaling is not None) and (scaling != \"none\"):\n",
    "        scaling = substitute_component_name(scaling, num_preproc_name_map)\n",
    "        steps.append(scaling)\n",
    "\n",
    "    return steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_pipeline(conf:dict)-> list:\n",
    "    # categorical numeric! text\n",
    "    pipeline = []\n",
    "\n",
    "    cat_data_preprocs = get_cat_data_preprocs(conf)\n",
    "    for p in cat_data_preprocs:\n",
    "        if p is not None:\n",
    "            pipeline.append(p)\n",
    "\n",
    "    num_data_preproc = get_num_data_preprocs(conf)\n",
    "    for p in num_data_preproc:\n",
    "        if p is not None:\n",
    "            pipeline.append(p)\n",
    "\n",
    "    text_data_preprocs = get_text_data_preprocs(conf)\n",
    "    for p in text_data_preprocs:\n",
    "        if p is not None:\n",
    "            pipeline.append(p)\n",
    "\n",
    "    feat_preproc = get_feat_preproc(conf)\n",
    "    if feat_preproc is not None:\n",
    "        pipeline.append(feat_preproc)\n",
    "\n",
    "    clf = get_classifier(conf)\n",
    "    pipeline.append(clf)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:38.740796Z",
     "start_time": "2023-05-17T14:16:38.696618Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pipeline_from_run_history(row : pandas.Series, rh : dict)-> list:\n",
    "    id_ = row[\"id\"]\n",
    "    pipeline = get_pipeline(rh[\"configs\"][str(id_)])\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:38.923084Z",
     "start_time": "2023-05-17T14:16:38.898559Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_component_types_cols(df: pandas.DataFrame) -> pandas.DataFrame:\n",
    "    def mapper(row: pandas.Series, c_names: list) -> list:\n",
    "        pipeline =row[\"pipeline\"]\n",
    "        return list(filter(lambda x: x in c_names, pipeline))\n",
    "\n",
    "    df[\"clfs\"] = df.apply(mapper, axis=1, args=(list(clfs_names_map.values()),))\n",
    "    df[\"n_clfs\"] = df[\"clfs\"].map(len)\n",
    "\n",
    "    df[\"feat_preproc\"] = df.apply(mapper, axis=1, args=(list(feat_preproc_name_map.values()),))\n",
    "    df[\"n_feat_preproc\"] = df[\"feat_preproc\"].map(len)\n",
    "\n",
    "    df[\"cat_preproc\"] = df.apply(mapper, axis=1, args=(list(cat_data_preproc_name_map.values()),))\n",
    "    df[\"n_cat_preproc\"] = df[\"cat_preproc\"].map(len)\n",
    "\n",
    "    df[\"text_preproc\"] = df.apply(mapper, axis=1, args=(list(text_data_preproc_name_map.values()),))\n",
    "    df[\"n_text_preproc\"] = df[\"text_preproc\"].map(len)\n",
    "\n",
    "    df[\"num_preproc\"] = df.apply(mapper, axis=1, args=(list(mum_data_preproc_name_map.values()),))\n",
    "    df[\"n_num_preproc\"] = df[\"num_preproc\"].map(len)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_edges(df: pandas.DataFrame) -> pandas.DataFrame:\n",
    "\n",
    "    def edges_builder(pipeline):\n",
    "        edges = []\n",
    "        edges.append([\"Input Data\", pipeline[0]])\n",
    "\n",
    "        for i in range(0, len(pipeline) -1):\n",
    "            edges.append(\n",
    "                [pipeline[i], pipeline[i+1]]\n",
    "            )\n",
    "        return edges\n",
    "\n",
    "    df[\"edges\"] = df[\"pipeline\"].map(edges_builder)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:39.097481Z",
     "start_time": "2023-05-17T14:16:39.094676Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_automl_obj(path: str):\n",
    "\n",
    "    with open(path, \"rb\") as infile:\n",
    "        return pickle.load(infile)\n",
    "\n",
    "\n",
    "def get_leaderboard(automl_obj) -> pandas.DataFrame:\n",
    "\n",
    "    lb = automl_obj.leaderboard()\n",
    "\n",
    "    return lb.sort_values(\"ensemble_weight\", ascending=False)\n",
    "\n",
    "\n",
    "def add_test_score_to_leaderboard(lb: pandas.DataFrame, test_data: pandas.DataFrame, automl_obj) -> pandas.DataFrame:\n",
    "    scores = []\n",
    "    X = test_data.drop(\"target\", axis=1)\n",
    "    y = test_data[\"target\"].values.ravel()\n",
    "    pipelines = automl_obj.get_models_with_weights()\n",
    "\n",
    "    for p in pipelines:\n",
    "        _,p = p\n",
    "        try:\n",
    "            pred = p.predict(X)\n",
    "            acc = sklearn.metrics.accuracy_score(y, pred)\n",
    "            scores.append(acc)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            scores.append(None)\n",
    "\n",
    "    lb[\"test_accuracy\"] = scores\n",
    "    return lb\n",
    "\n",
    "def get_test_data(d_name: str) -> pandas.DataFrame:\n",
    "    path = pjoin(ROOT, f\"datasets/{d_name}/test.csv\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:39.373259Z",
     "start_time": "2023-05-17T14:16:39.362576Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_pipeline(edges: list, out_name: str ,\n",
    "              save_dir: str = None,\n",
    "              graph_attrs: dict=None,\n",
    "              view: bool = False,\n",
    "              format=\"pdf\") -> graphviz.Digraph:\n",
    "\n",
    "    if graph_attrs is None:\n",
    "        graph_attr= {'rankdir':'LR'}\n",
    "\n",
    "    dot = graphviz.Digraph(out_name,\n",
    "                      graph_attr=graph_attr)\n",
    "    dot.format = format\n",
    "    dot.edge_attr.update(arrowhead='vee', arrowsize='1.4')\n",
    "\n",
    "    nodes = itertools.chain(*edges)\n",
    "    for n in nodes:\n",
    "        if \"Input Data\" in n:\n",
    "            dot.node(n,  shape=\"cylinder\", height=\"1.1\")\n",
    "        else:\n",
    "            dot.node(n, height = \"1.1\")\n",
    "\n",
    "    dot.edges(edges)\n",
    "\n",
    "    dot.render(directory=save_dir, view=view)\n",
    "    \n",
    "    \n",
    "    \n",
    "def visualize(run_df, name):\n",
    "    vis_dir = VISUALS.format(d_name=name, seed = SEED)\n",
    "\n",
    "    \n",
    "    for _, row in run_df.iterrows():\n",
    "        edges = row[\"edges\"]\n",
    "        try: \n",
    "            num_run = int(row[\"num_run\"])\n",
    "        except ValueError:\n",
    "            if np.isnan(row[\"num_run\"]):\n",
    "                num_run = \"no_run_\" + str(row[\"id\"])\n",
    "\n",
    "        in_ensemble = row[\"in_ensemble\"]\n",
    "\n",
    "        out_name = str(num_run) \n",
    "\n",
    "        if in_ensemble is True:\n",
    "            out_name = \"ens_\" + out_name \n",
    "\n",
    "\n",
    "        visualize_pipeline(edges = edges, out_name = out_name,\n",
    "                  save_dir = vis_dir, format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:16:40.359674Z",
     "start_time": "2023-05-17T14:16:40.358762Z"
    }
   },
   "outputs": [],
   "source": [
    "all_components = []\n",
    "\n",
    "for i in [\n",
    "    mum_data_preproc_name_map.values(),\n",
    "    text_data_preproc_name_map.values(),\n",
    "    cat_data_preproc_name_map.values(),\n",
    "    feat_preproc_name_map.values(),\n",
    "    clfs_names_map.values()]:\n",
    "    \n",
    "    all_components.extend(i)\n",
    "    \n",
    "dummy_components = pd.DataFrame(columns = all_components)\n",
    "\n",
    "\n",
    "def get_sum_components(runs: pandas.DataFrame, \n",
    "                       dummy_components: pandas.DataFrame = dummy_components) -> pandas.DataFrame:\n",
    "    \n",
    "    dummy = dummy_components.copy()\n",
    "    \n",
    "    for c in dummy:\n",
    "        dummy[c] = runs[\"pipeline\"].map(lambda x: 1 if c in x else 0)\n",
    "        \n",
    "    sums =dummy.sum()\n",
    "    sums = sums.to_frame()\n",
    "    sums = sums.reset_index(drop = False)\n",
    "    return sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Auto-Sklearn 2.0 <a id=ask1>\n",
    "\n",
    "[back to overview](#ov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  kc1<a id=1_kc1>\n",
    "    \n",
    "[back to overview](#ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summerize_run(name):\n",
    "    rh_path = RUN_HISTORY.format(d_name=name, seed= SEED)\n",
    "    \n",
    "    rh = load_run_history(rh_path)\n",
    "    run_df = create_run_df(rh)\n",
    "    \n",
    "    run_df[\"pipeline\"] = run_df.apply(get_pipeline_from_run_history, axis = 1, args=(rh,))\n",
    "    run_df[\"n_components\"] = run_df[\"pipeline\"].map(len)\n",
    "    run_df = add_component_types_cols(run_df)\n",
    "    run_df = build_edges(run_df)\n",
    "    \n",
    "    \n",
    "    run_df[\"success\"] = run_df[\"status\"].map(lambda x: 1 if x == \"StatusType.SUCCESS\" else 0)\n",
    "    run_df[\"timeout\"] = run_df[\"status\"].map(lambda x: 1 if x == \"StatusType.TIMEOUT\" else 0)\n",
    "    run_df[\"crashed\"] = run_df[\"status\"].map(lambda x: 1 if x == \"StatusType.CRASHED\" else 0)\n",
    "    run_df[\"stop\"] = run_df[\"status\"].map(lambda x: 1 if x == \"StatusType.STOP\" else 0)\n",
    "    run_df[\"memout\"] = run_df[\"status\"].map(lambda x: 1 if x == \"StatusType.MEMOUT\" else 0)\n",
    "    \n",
    "    \n",
    "    summary = run_df.groupby(\"n_components\").agg(\n",
    "    \n",
    "    n_sucess = (\"success\", \"sum\"),\n",
    "    n_timeout = (\"timeout\", \"sum\"),\n",
    "    n_crashed = (\"crashed\", \"sum\"),\n",
    "    n_stop = (\"stop\", \"sum\"),\n",
    "    n_memout = (\"memout\", \"sum\"),\n",
    "    )\n",
    "\n",
    "    summary = summary.reset_index(drop=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    automl_obj = get_automl_obj(AUTOML_OBJ.format(d_name = name, seed = SEED))\n",
    "\n",
    "    lb = get_leaderboard(automl_obj)\n",
    "\n",
    "    lb[\"type\"] = lb[\"type\"].map(clfs_names_map)\n",
    "    test = get_test_data(name)\n",
    "#     lb = add_test_score_to_leaderboard(lb, test, automl_obj)\n",
    "    \n",
    "#     run_df[\"num_run\"] = run_df[\"num_run\"].map(\n",
    "#             lambda x: None if np.isnan(x) else x\n",
    "#     )\n",
    "\n",
    "#     lb = lb.join(run_df[[\"num_run\", \"n_components\"]].set_index(\"num_run\"), on = \"model_id\")\n",
    "    \n",
    "    run_stats = pd.read_csv(\n",
    "        pjoin(DIR_TEMPLATE.format(d_name=name, seed=SEED),\n",
    "              \"run_stats.csv\"\n",
    "              )\n",
    "    )\n",
    "    run_df[\"in_ensemble\"] = run_df[\"id\"].map(lambda x: True if x in list(lb.index) else False)\n",
    "    \n",
    "    \n",
    "    return run_df, lb, run_stats, summary\n",
    "    \n",
    "    \n",
    "\n",
    "kc1_run_df, kc1_lb, kc1_run_stats, kc1_summary = summerize_run(\"kc1\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc1_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(kc1_run_df, \"kc1\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  electricity <a id=2_elec>\n",
    "[back to overview](#ov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_run_df, elec_lb, elec_run_stats, elec_summary = summerize_run(\"electricity\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_run_df[elec_run_df[\"status\"] == \"StatusType.SUCCESS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(elec_run_df, \"electricity\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  splice <a id=3_splice>\n",
    "[back to overview](#ov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splice_run_df, splice_lb, splice_run_stats, splice_summary = summerize_run(\"splice\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splice_run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splice_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(splice_run_df, \"splice\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. APSFailure <a id=aps>\n",
    "[back to overview](#ov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_run_df, aps_lb, aps_run_stats, aps_summary = summerize_run(\"APSFailure\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_run_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(aps_run_df, \"APSFailure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. volkert <a id=volker>\n",
    "[back to overview](#ov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_run_df, aps_lb, aps_run_stats, aps_summary = summerize_run(\"volkert\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(aps_run_df, \"volkert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6. volkert <a id=gas-drift>\n",
    "[back to overview](#ov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_run_df, gas_lb, gas_run_stats, gas_summary = summerize_run(\"gas-drift\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(gas_run_df, \"gas-drift\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autosklearn_37",
   "language": "python",
   "name": "autosklearn_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
